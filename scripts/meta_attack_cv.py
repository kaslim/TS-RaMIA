#!/usr/bin/env python3
"""
T53: å…ƒæ”»å‡»å™¨èåˆ - æ•´åˆå¤šä¸ªå¼±ç‰¹å¾ä¸ºå¼ºåˆ†ç±»å™¨
ä½¿ç”¨ 5-fold CVï¼ŒLogistic Regression / SVM
"""

import argparse
import pandas as pd
import numpy as np
import json
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

def select_features(df, exclude_patterns=['piece_id', 'split', 'is_member']):
    """é€‰æ‹©ç‰¹å¾åˆ—"""
    feature_cols = []
    for col in df.columns:
        # æ’é™¤ç‰¹å®šåˆ—
        if any(pat in col for pat in exclude_patterns):
            continue
        # åªä¿ç•™æ•°å€¼åˆ—
        if pd.api.types.is_numeric_dtype(df[col]):
            feature_cols.append(col)
    
    return feature_cols

def compute_low_fpr_metrics(y_true, y_score, fpr_thresholds=[0.001, 0.005, 0.01, 0.05, 0.1]):
    """è®¡ç®—ä½ FPR æŒ‡æ ‡"""
    fpr, tpr, thresholds = roc_curve(y_true, y_score)
    
    metrics = {}
    for target_fpr in fpr_thresholds:
        # æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡ FPR çš„ç‚¹
        idx = np.argmin(np.abs(fpr - target_fpr))
        actual_fpr = fpr[idx]
        actual_tpr = tpr[idx]
        
        metrics[f'TPR@{target_fpr*100:.1f}%FPR'] = {
            'tpr': float(actual_tpr),
            'fpr': float(actual_fpr),
            'advantage': float(actual_tpr - actual_fpr)
        }
    
    # Partial AUC (0-1%)
    mask = fpr <= 0.01
    if mask.sum() > 1:
        pauc = np.trapz(tpr[mask], fpr[mask])
        pauc_norm = pauc / 0.01  # æ ‡å‡†åŒ–
        metrics['pAUC(0-1%)'] = {
            'value': float(pauc),
            'normalized': float(pauc_norm)
        }
    
    return metrics

def train_and_evaluate(X, y, model_type='logreg', calibration='isotonic', n_folds=5, random_state=1337):
    """è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹"""
    
    # å‡†å¤‡å­˜å‚¨ç»“æœ
    fold_results = []
    all_y_true = []
    all_y_scores = []
    
    # 5-fold CV
    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)
    
    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):
        print(f"  Fold {fold_idx+1}/{n_folds}...", end=" ")
        
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y[train_idx], y[test_idx]
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # é€‰æ‹©æ¨¡å‹
        if model_type == 'logreg':
            base_model = LogisticRegression(
                penalty='l2',
                C=1.0,
                max_iter=1000,
                random_state=random_state
            )
        elif model_type == 'svm':
            base_model = SVC(
                kernel='rbf',
                C=1.0,
                probability=True,
                random_state=random_state
            )
        else:
            raise ValueError(f"Unknown model type: {model_type}")
        
        # æ ¡å‡†
        if calibration:
            model = CalibratedClassifierCV(
                base_model,
                method=calibration,
                cv=3
            )
        else:
            model = base_model
        
        # è®­ç»ƒ
        model.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹
        y_scores = model.predict_proba(X_test_scaled)[:, 1]
        
        # è¯„ä¼°
        auc = roc_auc_score(y_test, y_scores)
        low_fpr = compute_low_fpr_metrics(y_test, y_scores)
        
        fold_results.append({
            'fold': fold_idx + 1,
            'auc': float(auc),
            'n_train': len(y_train),
            'n_test': len(y_test),
            'low_fpr': low_fpr
        })
        
        all_y_true.extend(y_test)
        all_y_scores.extend(y_scores)
        
        print(f"AUC={auc:.4f}, TPR@1%={low_fpr['TPR@1.0%FPR']['tpr']*100:.1f}%")
    
    # æ€»ä½“æŒ‡æ ‡
    overall_auc = roc_auc_score(all_y_true, all_y_scores)
    overall_low_fpr = compute_low_fpr_metrics(all_y_true, all_y_scores)
    
    return fold_results, overall_auc, overall_low_fpr, all_y_true, all_y_scores

def main():
    parser = argparse.ArgumentParser(description='T53: å…ƒæ”»å‡»å™¨èåˆ')
    parser.add_argument('--csv', required=True, help='è¾“å…¥ CSVï¼ˆpiece-level ç‰¹å¾ï¼‰')
    parser.add_argument('--label_col', default='is_member', help='æ ‡ç­¾åˆ—')
    parser.add_argument('--models', nargs='+', default=['logreg'], 
                       choices=['logreg', 'svm'], help='æ¨¡å‹ç±»å‹')
    parser.add_argument('--calibration', default='isotonic', 
                       choices=['isotonic', 'sigmoid', 'none'], help='æ ¡å‡†æ–¹æ³•')
    parser.add_argument('--folds', type=int, default=5, help='CV æŠ˜æ•°')
    parser.add_argument('--out_json', required=True, help='è¾“å‡º JSON æŠ¥å‘Š')
    parser.add_argument('--out_png', required=True, help='è¾“å‡º ROC å›¾')
    parser.add_argument('--out_lowfpr', required=True, help='è¾“å‡ºä½ FPR æŒ‡æ ‡')
    parser.add_argument('--seed', type=int, default=1337, help='éšæœºç§å­')
    
    args = parser.parse_args()
    
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("T53: å…ƒæ”»å‡»å™¨èåˆï¼ˆå†²å‡» AUC 0.7ï¼‰")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print("")
    
    # åŠ è½½æ•°æ®
    df = pd.read_csv(args.csv)
    print(f"ğŸ“Š åŠ è½½æ•°æ®: {len(df)} ä¸ªä½œå“")
    print(f"  æˆå‘˜: {(df[args.label_col]==1).sum()}")
    print(f"  éæˆå‘˜: {(df[args.label_col]==0).sum()}")
    print("")
    
    # é€‰æ‹©ç‰¹å¾
    feature_cols = select_features(df)
    print(f"ğŸ¯ ç‰¹å¾é€‰æ‹©: {len(feature_cols)} ä¸ªç‰¹å¾")
    
    # ä¼˜å…ˆé€‰æ‹© TIS/PPL ç›¸å…³ç‰¹å¾
    priority_features = [c for c in feature_cols if any(x in c for x in ['tis', 'ppl', 'nll'])]
    if len(priority_features) > 50:
        priority_features = priority_features[:50]  # é™åˆ¶ç‰¹å¾æ•°
    
    print(f"   ä¼˜å…ˆç‰¹å¾: {len(priority_features)} ä¸ª")
    for i, feat in enumerate(priority_features[:10]):
        print(f"     {i+1}. {feat}")
    if len(priority_features) > 10:
        print(f"     ... è¿˜æœ‰ {len(priority_features)-10} ä¸ª")
    print("")
    
    # å‡†å¤‡æ•°æ®
    X = df[priority_features].fillna(0).values
    y = df[args.label_col].values
    
    # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
    results = {}
    best_auc = 0
    best_y_true = None
    best_y_scores = None
    
    for model_type in args.models:
        print(f"ğŸ”„ è®­ç»ƒæ¨¡å‹: {model_type.upper()}")
        
        calibration = args.calibration if args.calibration != 'none' else None
        
        fold_results, overall_auc, overall_low_fpr, y_true, y_scores = train_and_evaluate(
            X, y, 
            model_type=model_type,
            calibration=calibration,
            n_folds=args.folds,
            random_state=args.seed
        )
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        aucs = [r['auc'] for r in fold_results]
        mean_auc = np.mean(aucs)
        std_auc = np.std(aucs)
        
        results[model_type] = {
            'fold_results': fold_results,
            'mean_auc': float(mean_auc),
            'std_auc': float(std_auc),
            'overall_auc': float(overall_auc),
            'overall_low_fpr': overall_low_fpr
        }
        
        print(f"  âœ“ å¹³å‡ AUC: {mean_auc:.4f} Â± {std_auc:.4f}")
        print(f"    æ€»ä½“ AUC: {overall_auc:.4f}")
        print(f"    TPR@1%FPR: {overall_low_fpr['TPR@1.0%FPR']['tpr']*100:.1f}%")
        print("")
        
        # è®°å½•æœ€ä½³
        if overall_auc > best_auc:
            best_auc = overall_auc
            best_y_true = y_true
            best_y_scores = y_scores
    
    # ä¿å­˜ JSON æŠ¥å‘Š
    summary = {
        'n_samples': len(df),
        'n_features': len(priority_features),
        'n_folds': args.folds,
        'calibration': args.calibration,
        'models': results,
        'best_model': max(results.items(), key=lambda x: x[1]['overall_auc'])[0],
        'best_auc': float(best_auc)
    }
    
    with open(args.out_json, 'w') as f:
        json.dump(summary, f, indent=2)
    print(f"âœ… JSON æŠ¥å‘Šå·²ä¿å­˜: {args.out_json}")
    
    # ä¿å­˜ä½ FPR æŒ‡æ ‡
    best_model = summary['best_model']
    with open(args.out_lowfpr, 'w') as f:
        json.dump(results[best_model]['overall_low_fpr'], f, indent=2)
    print(f"âœ… ä½ FPR æŒ‡æ ‡å·²ä¿å­˜: {args.out_lowfpr}")
    
    # ç»˜åˆ¶ ROC æ›²çº¿
    if best_y_true is not None:
        plt.figure(figsize=(8, 6))
        fpr, tpr, _ = roc_curve(best_y_true, best_y_scores)
        plt.plot(fpr, tpr, linewidth=2, label=f'Meta-Attack (AUC={best_auc:.3f})')
        plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title(f'T53: Meta-Attack ROC ({best_model.upper()})', fontsize=14)
        plt.legend(loc='lower right', fontsize=10)
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig(args.out_png, dpi=150)
        plt.close()
        print(f"âœ… ROC æ›²çº¿å·²ä¿å­˜: {args.out_png}")
    
    print("")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"âœ… T53 å®Œæˆï¼æœ€ä½³æ¨¡å‹: {best_model.upper()}, AUC={best_auc:.4f}")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

if __name__ == '__main__':
    main()

